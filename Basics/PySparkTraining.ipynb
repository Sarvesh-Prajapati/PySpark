{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Install PySpark**"
      ],
      "metadata": {
        "id": "9PK4vnvf_ROT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwVLRlBh-NgM",
        "outputId": "a4dd6514-377c-47ba-f637-597acd850db2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create a Spark session**"
      ],
      "metadata": {
        "id": "SVQmLaD__WYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DGZhQoeE8H0u"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master('local').appName('PySpark_HandsOn').getOrCreate()  # A SparkSession is the entry point to using Spark."
      ],
      "metadata": {
        "id": "Lbvz5LlqPdtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MK6KnGBG9MNa",
        "outputId": "b47b4eae-27ed-47fb-83c9-b3e1664d9080"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get data from 'Sample' folder on LHS panel of Colab screen**"
      ],
      "metadata": {
        "id": "H3ZCeAKS_dH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing_df = spark.read.csv('/content/sample_data/california_housing_test.csv')"
      ],
      "metadata": {
        "id": "M2JErF1b9SZr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the above cell, Spark read the .csv file without discerning the header row & schema hence** ```printSchema()``` **here doesn't show the name of headers in output:**"
      ],
      "metadata": {
        "id": "ATiqQb6o_t3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m19JbrPu9ymN",
        "outputId": "9b2639aa-aba1-44f8-e47c-c1c2297e906f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-importing the file and specifying the properties** ```header``` **and** ```inferSchema``` **this time:**"
      ],
      "metadata": {
        "id": "TbS7ZJQrAIoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing_df = spark.read.csv('/content/sample_data/california_housing_test.csv', header = True, inferSchema = True)"
      ],
      "metadata": {
        "id": "1o6CANPy94V8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing_df.printSchema() # printSchema() here shows the col names now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slz882ZZ9_VM",
        "outputId": "640634d7-fc14-4680-f2da-b48c6a9f064a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Viewing the frame's content:**"
      ],
      "metadata": {
        "id": "yGbBMxOWA8qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn6_NqfT-VuT",
        "outputId": "2692ae55-964a-43eb-ff0d-b686593b0e67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
            "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filtering the frame:**"
      ],
      "metadata": {
        "id": "hnEcicyqBAm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing_df.filter(cali_housing_df['housing_median_age'] > 30).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju9vskMj-dJB",
        "outputId": "1fbf8023-0f1e-44d4-9edf-be2654f30086"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
            "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
            "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
            "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "cali_housing_df.filter(col('housing_median_age') > 30).show(5)\n",
        "# cali_housing_df.filter(col('housing_median_age') > 30)[['longitude', 'latitude', 'housing_median_age', 'median_income']].show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3TNcQDc_EWS",
        "outputId": "8523baba-4b9a-4b81-e475-41168a10fb92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
            "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
            "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
            "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "cali_housing_df.agg(F.min(col('median_house_value')),\n",
        "                    F.max(col('median_house_value')),\n",
        "                    F.count(col('median_house_value')),\n",
        "                    F.sum(col('median_house_value')),\n",
        "                    F.avg(col('median_house_value')),\n",
        "                    F.stddev(col('median_house_value'))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsfLyI2dBEUY",
        "outputId": "833b6bc4-0c2c-46bc-b174-a2acf19914bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------------+\n",
            "|min(median_house_value)|max(median_house_value)|count(median_house_value)|sum(median_house_value)|avg(median_house_value)|stddev(median_house_value)|\n",
            "+-----------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------------+\n",
            "|                22500.0|               500001.0|                     3000|           6.17538825E8|             205846.275|        113119.68746964622|\n",
            "+-----------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating spark dataframes**"
      ],
      "metadata": {
        "id": "rO-d3SpVPk79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataframe from dictionary"
      ],
      "metadata": {
        "id": "j7cd8J5UChe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary data\n",
        "data = {'ID' : [1, 2, 3, 4, 5],\n",
        "        'Name' : ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "        'Department' : ['Sales', 'HR', 'IT', 'Sales', 'IT'],\n",
        "        'Salary' : [3000, 4000, 5000, 4500, 6000]\n",
        "        }"
      ],
      "metadata": {
        "id": "Ef-icghh9lDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are two ways to create a spark dataframe from a dictionary of data. First way is to create an intermediate dataframe using pandas and then passing that frame to** ```spark.createDataFrame()``` **to create a spark dataframe. Second, way is to zip the dictionary values into tuples; each tuple serves as a record for spark dataframe.**"
      ],
      "metadata": {
        "id": "bD_Z9x_DBZoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "OZsS6EtVQuIK"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "lN2IJTFJAZgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ WAY-1 ----------------------\n",
        "import pandas as pd\n",
        "pd_data = pd.DataFrame(data)\n",
        "df = spark.createDataFrame(pd_data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LQSpq61B52m",
        "outputId": "e4dd52ee-ad60-48b1-8c5b-2a2bb084aac4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| ID|   Name|Department|Salary|\n",
            "+---+-------+----------+------+\n",
            "|  1|  Alice|     Sales|  3000|\n",
            "|  2|    Bob|        HR|  4000|\n",
            "|  3|Charlie|        IT|  5000|\n",
            "|  4|  David|     Sales|  4500|\n",
            "|  5|    Eve|        IT|  6000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data = {'ID' : [1, 2, 3, 4, 5],\n",
        "#         'Name' : ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "#         'Department' : ['Sales', 'HR', 'IT', 'Sales', 'IT'],\n",
        "#         'Salary' : [3000, 4000, 5000, 4500, 6000]\n",
        "#         }\n",
        "# list(zip(*data.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzDQC9k0D1UL",
        "outputId": "0c55669f-79eb-4c96-d6ab-d320aa8c0ca0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Alice', 'Sales', 3000),\n",
              " (2, 'Bob', 'HR', 4000),\n",
              " (3, 'Charlie', 'IT', 5000),\n",
              " (4, 'David', 'Sales', 4500),\n",
              " (5, 'Eve', 'IT', 6000)]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ WAY-2 ----------------------\n",
        "tuple_data = list(zip(*data.values()))  # See the rough work done in prev cell\n",
        "cols = list(data.keys())  # ['ID', 'Name', 'Department', 'Salary']\n",
        "df = spark.createDataFrame(tuple_data, cols)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHoG0QH-CHOQ",
        "outputId": "d28fd4bf-c5d1-4c88-da23-67f2e2fc5792"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| ID|   Name|Department|Salary|\n",
            "+---+-------+----------+------+\n",
            "|  1|  Alice|     Sales|  3000|\n",
            "|  2|    Bob|        HR|  4000|\n",
            "|  3|Charlie|        IT|  5000|\n",
            "|  4|  David|     Sales|  4500|\n",
            "|  5|    Eve|        IT|  6000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataframe from list of tuples (and performing basic analysis)"
      ],
      "metadata": {
        "id": "f1H9O14CFd6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [(1, \"Alice\", \"Sales\", 3000),\n",
        "        (2, \"Bob\", \"HR\", 4000),\n",
        "        (3, \"Charlie\", \"IT\", 5000),\n",
        "        (4, \"David\", \"Sales\", 4500),\n",
        "        (5, \"Eve\", \"IT\", 6000) ]\n",
        "\n",
        "cols = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, cols)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHUGJ578PqV6",
        "outputId": "952a56a0-5180-4a27-a8b1-55c3de4c55c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| ID|   Name|Department|Salary|\n",
            "+---+-------+----------+------+\n",
            "|  1|  Alice|     Sales|  3000|\n",
            "|  2|    Bob|        HR|  4000|\n",
            "|  3|Charlie|        IT|  5000|\n",
            "|  4|  David|     Sales|  4500|\n",
            "|  5|    Eve|        IT|  6000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count() # 5\n",
        "df.describe().show()  # Statistics about entire df\n",
        "df.describe('Salary').show() # Statistics of the 'Salary' col"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXK0ean3l18a",
        "outputId": "8e4924c5-4dcc-4d6e-eb01-3c0f2029d28a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+\n",
            "|summary|           Salary|\n",
            "+-------+-----------------+\n",
            "|  count|                5|\n",
            "|   mean|           4500.0|\n",
            "| stddev|1118.033988749895|\n",
            "|    min|             3000|\n",
            "|    max|             6000|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average salary of each dept\n",
        "df.groupby('Department').agg({'Salary' : 'avg'}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tyq9fsmmVfD",
        "outputId": "8f97dccd-073b-4894-8f93-483fb3ef857c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3750.0|\n",
            "|        HR|     4000.0|\n",
            "|        IT|     5500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Employees earning more than 4000\n",
        "df.filter(col('Salary') > 4000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y2nZRyYmqqP",
        "outputId": "06a49e3f-aef8-4cf6-ab2e-a668a24e3b17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| ID|   Name|Department|Salary|\n",
            "+---+-------+----------+------+\n",
            "|  3|Charlie|        IT|  5000|\n",
            "|  4|  David|     Sales|  4500|\n",
            "|  5|    Eve|        IT|  6000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting employees by their salary in descending order\n",
        "df.sort(col('Salary').desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCeqrlWjm8Nw",
        "outputId": "d388723f-1c42-473c-8e93-46637fafe662"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------+------+\n",
            "| ID|   Name|Department|Salary|\n",
            "+---+-------+----------+------+\n",
            "|  5|    Eve|        IT|  6000|\n",
            "|  3|Charlie|        IT|  5000|\n",
            "|  4|  David|     Sales|  4500|\n",
            "|  2|    Bob|        HR|  4000|\n",
            "|  1|  Alice|     Sales|  3000|\n",
            "+---+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop() # Good practice to run this line after ALL the Spark jobs have finished"
      ],
      "metadata": {
        "id": "kzkgffp3pnng"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ingesting data from a GitHub link into Spark dataframe**"
      ],
      "metadata": {
        "id": "QBBotwANpz36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The method** ```spark.read.csv()``` **doesn't directly fetch an internet file's content into Spark. First, the data is ingested into Pandas frame and then that frame's reference is passed to a Spark frame:**"
      ],
      "metadata": {
        "id": "pPCr_2_ysWAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "csv_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
        "\n",
        "# Read the CSV data from the URL into a Pandas DataFrame\n",
        "pandas_df = pd.read_csv(csv_url)\n",
        "\n",
        "# Convert the Pandas DataFrame into a Spark DataFrame\n",
        "iris_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "iris_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ogWhyvWp9em",
        "outputId": "4540ead3-11ba-4dc3-b56d-15c93cd6f96c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In order to fetch the data hosted on the internet directly into Spark frame using** ```spark.read.csv()``` **method, you've to first download that data to your local machine or into Colab and refer the downloaded file's URL into the method:**"
      ],
      "metadata": {
        "id": "H5WyaVvis_sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('IrisDataIngestion').getOrCreate()\n"
      ],
      "metadata": {
        "id": "nEZxbHdVtgjG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We use** ```requests.get()``` **to download the file and save it.**"
      ],
      "metadata": {
        "id": "BSXOGWG-uFnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
        "csv_path = \"/content/iris.csv\"  # Saving file in Colab's local storage\n",
        "\n",
        "# Downloading the file\n",
        "response = requests.get(csv_url)    # 'requests.get()' sends HTTP GET request to specified URL & retrieves data (webpages, APIs, files like CSVs/JSONs) from server.\n",
        "with open(csv_path, 'wb') as file:  # 'w' (write mode) is used for text files (expects strings); 'wb' (write-binary mode) is used when downloading binary content (like CSV, images, or PDFs) to ensure it is saved correctly.\n",
        "    file.write(response.content)    # writing the content of file saved in 'response' to the file\n",
        "\n",
        "# Check if the file was downloaded successfully\n",
        "os.path.exists(csv_path)  # o/p: True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMEbyC5NuMHh",
        "outputId": "2c3da425-d081-42f9-c765-3ffc2d1249c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, we read the local CSV file using PySpark's** ```spark.read.csv()```."
      ],
      "metadata": {
        "id": "rX93NQnnuz3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(csv_path, header = True, inferSchema = True)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsEHSN_WurB3",
        "outputId": "748439fe-5d18-43ee-f2ba-e655fc4b7f9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}